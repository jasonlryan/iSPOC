{
  "id": "IT10",
  "title": "Use of Generative Artificial Intelligence AI Systems Policy",
  "filename": "IT10 Use of Generative Artificial Intelligence AI Systems Policy.docx",
  "extracted_date": "2025-04-22",
  "metadata": {},
  "full_text": "Introduction\nThis policy outlines the guidelines for the appropriate use of AI systems, such as ChatGPT, within MHA. These guidelines are established to ensure the responsible and ethical use of AI to benefit our residents, tenants, members, colleagues, and the overall organization.\nAI-generated content should be viewed as a starting point, not the finished product. While AI can provide a valuable tool for generating content, it cannot replace the creativity and critical thinking skills of human writers and editors.+\nThis policy is primarily focused on the use of personal assistant AI tools, such as ChatGPT. These tools are leveraged to assist in various tasks, including content generation, administrative support, and communication enhancement. The guidelines ensure that while these tools offer efficiency and support, they are used responsibly and do not replace the critical role of human judgment and interaction.\nScope and Purpose\nThis policy applies to all colleagues, volunteers, contractors, and other personnel who use generative AI technology in the course of their work for MHA. The purpose of this policy is to:\nProvide a framework for the ethical and responsible use of AI within MHA.\nPromote the well-being, safety, and privacy of colleagues, residents, tenants, and members.\nEnsure AI technology is not seen as a replacement for essential human interaction.\nAddress potential risks and concerns associated with AI usage.\nThis policy aims to provide guidelines for the responsible use of AI to generate content, emphasising the need for proofing, editing, fact-checking, and using AI-generated content as a starting point, not the finished product. This includes services such as Text to Voice services that use AI to create a more real tone to speech.\nDefinitions\nUse of Generative AI at MHA\nGenerative AI technologies are trained on massive datasets of text from the internet, including books, articles, and websites, and it uses deep learning techniques to generate responses that are contextually relevant and grammatically correct. It can be used for a variety of natural language processing tasks, such as language translation, summarisation, and conversation generation.\nOne of the most significant advantages of generative AI is its ability to generate natural-sounding responses that are often difficult to distinguish from human-generated text. This makes it an excellent tool for conversational applications, such as chatbots, customer service, and virtual assistants. \nHowever, like all AI technologies, there are also concerns about the ethical implications of generative AI's use, particularly in areas such as bias, privacy, and the potential for misuse. As such, responsible use of generative AI technologies, along with appropriate regulation and oversight, is essential to ensure that it is used for good and benefits society.\nMHA recognises the potential benefits of using AI to improve efficiency and productivity in the workplace. This includes tasks such as generating and summarising text or content for reports, emails, presentations, images, and marketing and fundraising communications. However, we also acknowledge the importance of using AI responsibly, lawfully, and ethically, particularly when it comes to generating content. \nGenerative AI technologies are designed to process and generate language based on the input it receives. While tools such as ChatGPT and Google Gemini are sophisticated tools that can provide helpful insights and responses, they are not inherently equipped to manage sensitive information such as any personal information covered by GDPR.\nSince generative AI technologies are digital tools that can potentially be accessed by others, it is not permitted to share personal and/or confidential information with it. There is a risk that the information could be exposed or misused, either through a security breach or by unintended parties gaining access. Using personal and/or confidential information could therefore lead to a breach of GDPR.\nAdditionally, generative AI technologies are often not legal entities and, therefore, are not bound by the same confidentiality agreements or legal protections as human employees or contractors. As such, it will not be able to guarantee the same level of discretion and confidentiality that a trusted human partner could provide. \nWhen using generative AI, the three need to be considered:\nHow your question or information will be used by the system. These systems learn based on the information you enter. Just as you would not share work documents on social media sites, do not input such material into generative AI tools.\nHow answers from generative AI can mislead. These tools can produce credible looking output. They can also offer different responses to the same question if it is posed more than once, and they may derive their answers from sources you would not trust in other contexts. Therefore, be aware of the potential for misinformation from these systems. Always apply the high standards of rigour you would to anything you produce, and reference where you have sourced output from one of these tools.\nHow generative AI operates. A generative AI tool, such as an LLM (see definitions), will answer your question by probabilistically choosing words from a series of options it classifies as plausible. These tools cannot understand context or bias. Always treat with caution the outputs these tools produce and challenge the outputs using your own judgement and knowledge.\nResponsible Use of AI\nRespect: AI systems should not replace human interaction with residents, tenants, and members but complement it. \nExternal Partners: When collaborating with external partners in AI development or data sharing, we will ensure they adhere to this policy and other associated policies and procedures.\nRisk Management\nAppropriate controls should be in place to manage the risk associated with the use of AI.\nEthical and Safeguarding Considerations\nThe use of AI should be done in line with MHA’s safeguarding policy.\nTransparency: We will be transparent about the use of AI across all areas of MHA, ensuring residents and their families understand how AI technologies are employed to enhance care.\nPrivacy: We will prioritize the privacy and data security of residents, employees, and visitors when collecting and processing data with AI technologies. All data will be managed in compliance with applicable data protection regulations. Refer to MHA’s privacy notices for mor information [Information Governance Policies].\nBias and Discrimination: Care must be taken to avoid AI systems perpetuating biases, stereotypes, or discrimination. \nClinical and End-of-Life Care: AI should never be used to deliver end-of-life care or make critical medical decisions. Human judgment and compassion are irreplaceable.\nConsent: Users should be mindful of the information they are putting into AI systems, ensuring that consent is given if required. Refer to the Consent policy for more information.\nData Governance\nData Collection: We will only collect data that is necessary for the improvement of the care and support of people who use MHA services, and we will obtain informed consent where required.\nData Retention: Data collected by AI systems should be retained only as long as necessary for the intended purpose, and securely deleted when no longer needed [IG001a].\nData Security: Stringent security measures will be in place to protect data from unauthorized access or breaches.\nColleagues are required to report any concerns related to AI usage, including privacy breaches, bias, or ethical issues, to management or a designated authority.\nCaution: Never share personal, sensitive, or confidential data with generative AI technologies. Any use of AI that COULD impact on personal data MUST have a DPIA completed\nRequirements for AI Generated Content\nProofing: All AI-generated content must be proofread and checked for accuracy by a human before being published or shared. This includes checking for spelling errors, grammar mistakes, and factual inaccuracies.\nEditing: AI-generated content must be edited to ensure that it is well-written, coherent, and engaging. This includes ensuring that the content is structured in a logical manner and that it is appropriate for the intended audience, aligned to MHA’s brand guidelines and tone of voice.\n\nFact-checking: AI-generated content must be fact-checked via a reputable source to ensure that all information is accurate and up to date. This includes verifying sources, checking statistics, and ensuring that any claims made in the content are supported by evidence.\n\nAI is the starting point, not the finished product.\nRoles and Responsibilities\nContinuous Improvement\nMHA will regularly review and assess the impact of AI on the people who MHA support and make necessary adjustments to improve outcomes and ensure ethical use.\nResidents, members, families, representatives, and colleagues will have avenues to report concerns, issues, or suggestions related to AI use.\nTraining and Monitoring\nActions which are found to contravene those outlined in this policy will be referred into the appropriate process for resolution, which may include the Disciplinary or Probation Policy.\nCompliance is assessed through direct observation, monitoring, and supervision of our colleagues and associated systems.\nCommunication and Dissemination\nThis policy is disseminated and implemented within all MHA services through MHA’s channels of communication.\nEach colleague’s line manager must ensure that all teams are aware of their roles, responsibilities.\nThis policy will be available to the people we support and their representatives in alternate formats, as required.\nAny review of this policy will include consultation with our colleagues, review of support planning, incident reports, quality audits and feedback from other agencies.\nQueries and issues relating to this policy should be referred to the Standards and Policy Team policies@mha.org.uk \nResources\nMHA policy documents, procedures, and guidance:\nRisk Management Policy \nComputer Use Policy \nSafeguarding Policy \nMHA’s Privacy Notices [Information Governance Policies]\nConsent Policy \nDisciplinary Policy \nProbation Policy\nRetention Schedule Guidance \nAppendices \nAppendix 1 – use of AI notetakers and secretarial tools\nAppendix 1 - Use of AI Notetakers and Secretarial Tools\nIntroduction\nMHA’s chosen Artificial Intelligence (AI) notetaker and secretarial tool is Otter.ai.  This policy establishes guidelines for the use of Otter.ai (Otter hereafter) at MHA to ensure compliance with the Data Protection Act 2018 and the UK General Data Protection Regulation (GDPR), the Data Protection Legislation, alignment with MHA’s organisational values and to control the cost of licencing.  It aims to ensure that Otter is used appropriately for business purposes while safeguarding the privacy and personal information of all individuals.  No other AI notetaker or secretarial tools may be used by any colleague for business purposes unless with the explicit written consent of this policy owner.\nApproved Use Cases for Otter.ai \nOtter must not be used for highly sensitive meetings or meetings where a large amount of commercially confidential information is shared. This would normally include HR process meetings (such as investigations and hearings), Executive (ELT), Board and Committee meetings.\nOtter must only be used for business meetings discussing business processes or operational matters.  This can include both internal and external meetings.  \nOtter must not be used for notetaking in meetings where person-identifiable information could be discussed. This includes, but is not limited to, personal information relating to colleagues, residents, members, and volunteers. This is because Otter automatically creates a record of everything discussed, including any sensitive or confidential information shared. Unlike traditional meeting notes, the content captured by Otter is stored and processed externally, meaning it cannot be considered privileged or protected in the same way. \nOtter must not be used to record colleague 1:1 meetings. \nAll meeting attendees should be advised that an AI notetaker is being used to transcribe, record, analyse and minute the meeting.  This should be done at the start of each meeting where Otter is used.\nOtter may only be used by the meeting Chair – meeting delegates must not bring their own Otter to a meeting. \nOtter users must not send their Otter to a meeting to attend on their behalf. \nMaintaining the authorised user’s Otter.ai account\nAuthorised users of Otter must: \nRetain their account within MHA’s Microsoft environment and not link it to other apps or accounts such as Gmail or Slack. \nComplete their full profile in the Account Setting tab.\nRemain part of the centrally managed Otter workspace (mha.org.uk).\nSet their notetaker permissions so that Otter does not automatically join every meeting they organise or attend. Instead, users should manually select which meetings Otter should join, considering this policy. \nNot use the ‘refer and earn’ function. \nRetain the default language for conversations as English. \nSet the default audience for shared notes to ‘do not share’.\nTurn off Otter Pilot chat messages.\nTurn off auto capture meeting screens.\nEnsure that Two Factor Authentication remains turned on. \nConsideration should be made as to when any Otter owner leaves MHA whether any notes stored in Otter should be retained in accordance with MHA’s Retention Schedule Guidance.  Leaver accounts will be closed and all stored data permanently deleted 30 days after an Otter user leaves MHA.\nData Protection and Legislation Compliance\nAll colleagues must ensure that Otter is used in a manner compliant with the Date Protection Legislation as detailed in MHA’s Information Governance policies and the guidelines in this policy.  This includes avoiding the input or recording of any personal data related to individuals in Otter -transcribed content.\nDuring meetings where Otter is used, delegates and the Chair must continuously consider the Date Protection Legislation and should where possible aim to avoid any incidences of referring to information which could identify a person – for example it would be best practice to avoid using surnames of people. \nRegular reviews and audits will be conducted to ensure compliance with MHA’s Information Governance policies and this policy document.\nLicensing and Access \nMHA will provide Otter Business (paid-for) licences to colleagues where a clear business need is identified.  These licences will be centrally managed and funded by MHA. \nOtter Business licences will only be issued to colleagues who chair 10 or more hours of in-scope (business process) meetings per week.  \nUnder no circumstances may colleagues subscribe to or purchase a paid-for Otter licence independently, including signing up for a trial offer.  This includes using the free version of Otter as MHA would be automatically charged a full licence fee if the user were to exceed the free account usage limits. \nNo colleague may use any version of Otter which has not been issued by MHA’s IT department or is not registered to the user’s MHA Microsoft account.  This includes Otter on personal devices and instances paid for personally. \nVersion Control",
  "sections": {
    "summary": "MHA’s chosen Artificial Intelligence (AI) notetaker and secretarial tool is Otter.ai. This policy establishes guidelines for the use of Otter.ai (Otter hereafter) at MHA to ensure compliance with the Data Protection Act 2018 and the UK General Data Protection Regulation (GDPR), the Data Protection Legislation, alignment with MHA’s organisational values and to control the cost of licencing. It aims to ensure that Otter is used appropriately for business purposes while safeguarding the privacy and personal information of all individuals. No other AI notetaker or secretarial tools may be used by any colleague for business purposes unless with the explicit written consent of this policy owner. Approved Use Cases for Otter.ai Otter must not be used for highly sensitive meetings or meetings where a large amount of commercially confidential information is shared. This would normally include HR process meetings (such as investigations and hearings), Executive (ELT), Board and Committee meetings. Otter must only be used for business meetings discussing business processes or operational matters. This can include both internal and external meetings. Otter must not be used for notetaking in meetings where person-identifiable information could be discussed. This includes, but is not limited to, personal information relating to colleagues, residents, members, and volunteers. This is because Otter automatically creates a record of everything discussed, including any sensitive or confidential information shared. Unlike traditional meeting notes, the content captured by Otter is stored and processed externally, meaning it cannot be considered privileged or protected in the same way. Otter must not be used to record colleague 1:1 meetings. All meeting attendees should be advised that an AI notetaker is being used to transcribe, record, analyse and minute the meeting. This should be done at the start of each meeting where Otter is used. Otter may only be used by the meeting Chair – meeting delegates must not bring their own Otter to a meeting. Otter users must not send their Otter to a meeting to attend on their behalf. Maintaining the authorised user’s Otter.ai account Authorised users of Otter must: Retain their account within MHA’s Microsoft environment and not link it to other apps or accounts such as Gmail or Slack. Complete their full profile in the Account Setting tab. Remain part of the centrally managed Otter workspace (mha.org.uk). Set their notetaker permissions so that Otter does not automatically join every meeting they organise or attend. Instead, users should manually select which meetings Otter should join, considering this policy. Not use the ‘refer and earn’ function. Retain the default language for conversations as English. Set the default audience for shared notes to ‘do not share’. Turn off Otter Pilot chat messages. Turn off auto capture meeting screens. Ensure that Two Factor Authentication remains turned on. Consideration should be made as to when any Otter owner leaves MHA whether any notes stored in Otter should be retained in accordance with MHA’s Retention Schedule Guidance. Leaver accounts will be closed and all stored data permanently deleted 30 days after an Otter user leaves MHA. Data Protection and Legislation Compliance All colleagues must ensure that Otter is used in a manner compliant with the Date Protection Legislation as detailed in MHA’s Information Governance policies and the guidelines in this policy. This includes avoiding the input or recording of any personal data related to individuals in Otter -transcribed content. During meetings where Otter is used, delegates and the Chair must continuously consider the Date Protection Legislation and should where possible aim to avoid any incidences of referring to information which could identify a person – for example it would be best practice to avoid using surnames of people. Regular reviews and audits will be conducted to ensure compliance with MHA’s Information Governance policies and this policy document. Licensing and Access MHA will provide Otter Business (paid-for) licences to colleagues where a clear business need is identified. These licences will be centrally managed and funded by MHA. Otter Business licences will only be issued to colleagues who chair 10 or more hours of in-scope (business process) meetings per week. Under no circumstances may colleagues subscribe to or purchase a paid-for Otter licence independently, including signing up for a trial offer. This includes using the free version of Otter as MHA would be automatically charged a full licence fee if the user were to exceed the free account usage limits. No colleague may use any version of Otter which has not been issued by MHA’s IT department or is not registered to the user’s MHA Microsoft account. This includes Otter on personal devices and instances paid for personally. Version Control",
    "purpose": "This policy applies to all colleagues, volunteers, contractors, and other personnel who use generative AI technology in the course of their work for MHA. The purpose of this policy is to: Provide a framework for the ethical and responsible use of AI within MHA. Promote the well-being, safety, and privacy of colleagues, residents, tenants, and members. Ensure AI technology is not seen as a replacement for essential human interaction. Address potential risks and concerns associated with AI usage. This policy aims to provide guidelines for the responsible use of AI to generate content, emphasising the need for proofing, editing, fact-checking, and using AI-generated content as a starting point, not the finished product. This includes services such as Text to Voice services that use AI to create a more real tone to speech.",
    "definitions": "Use of Generative AI at MHA Generative AI technologies are trained on massive datasets of text from the internet, including books, articles, and websites, and it uses deep learning techniques to generate responses that are contextually relevant and grammatically correct. It can be used for a variety of natural language processing tasks, such as language translation, summarisation, and conversation generation. One of the most significant advantages of generative AI is its ability to generate natural-sounding responses that are often difficult to distinguish from human-generated text. This makes it an excellent tool for conversational applications, such as chatbots, customer service, and virtual assistants. However, like all AI technologies, there are also concerns about the ethical implications of generative AI's use, particularly in areas such as bias, privacy, and the potential for misuse. As such, responsible use of generative AI technologies, along with appropriate regulation and oversight, is essential to ensure that it is used for good and benefits society. MHA recognises the potential benefits of using AI to improve efficiency and productivity in the workplace. This includes tasks such as generating and summarising text or content for reports, emails, presentations, images, and marketing and fundraising communications. However, we also acknowledge the importance of using AI responsibly, lawfully, and ethically, particularly when it comes to generating content. Generative AI technologies are designed to process and generate language based on the input it receives. While tools such as ChatGPT and Google Gemini are sophisticated tools that can provide helpful insights and responses, they are not inherently equipped to manage sensitive information such as any personal information covered by GDPR. Since generative AI technologies are digital tools that can potentially be accessed by others, it is not permitted to share personal and/or confidential information with it. There is a risk that the information could be exposed or misused, either through a security breach or by unintended parties gaining access. Using personal and/or confidential information could therefore lead to a breach of GDPR. Additionally, generative AI technologies are often not legal entities and, therefore, are not bound by the same confidentiality agreements or legal protections as human employees or contractors. As such, it will not be able to guarantee the same level of discretion and confidentiality that a trusted human partner could provide. When using generative AI, the three need to be considered: How your question or information will be used by the system. These systems learn based on the information you enter. Just as you would not share work documents on social media sites, do not input such material into generative AI tools. How answers from generative AI can mislead. These tools can produce credible looking output. They can also offer different responses to the same question if it is posed more than once, and they may derive their answers from sources you would not trust in other contexts. Therefore, be aware of the potential for misinformation from these systems. Always apply the high standards of rigour you would to anything you produce, and reference where you have sourced output from one of these tools. How generative AI operates. A generative AI tool, such as an LLM (see definitions), will answer your question by probabilistically choosing words from a series of options it classifies as plausible. These tools cannot understand context or bias. Always treat with caution the outputs these tools produce and challenge the outputs using your own judgement and knowledge. Responsible Use of AI Respect: AI systems should not replace human interaction with residents, tenants, and members but complement it. External Partners: When collaborating with external partners in AI development or data sharing, we will ensure they adhere to this policy and other associated policies and procedures. Risk Management Appropriate controls should be in place to manage the risk associated with the use of AI. Ethical and Safeguarding Considerations The use of AI should be done in line with MHA’s safeguarding policy. Transparency: We will be transparent about the use of AI across all areas of MHA, ensuring residents and their families understand how AI technologies are employed to enhance care. Privacy: We will prioritize the privacy and data security of residents, employees, and visitors when collecting and processing data with AI technologies. All data will be managed in compliance with applicable data protection regulations. Refer to MHA’s privacy notices for mor information [Information Governance Policies]. Bias and Discrimination: Care must be taken to avoid AI systems perpetuating biases, stereotypes, or discrimination. Clinical and End-of-Life Care: AI should never be used to deliver end-of-life care or make critical medical decisions. Human judgment and compassion are irreplaceable. Consent: Users should be mindful of the information they are putting into AI systems, ensuring that consent is given if required. Refer to the Consent policy for more information. Data Governance Data Collection: We will only collect data that is necessary for the improvement of the care and support of people who use MHA services, and we will obtain informed consent where required. Data Retention: Data collected by AI systems should be retained only as long as necessary for the intended purpose, and securely deleted when no longer needed [IG001a]. Data Security: Stringent security measures will be in place to protect data from unauthorized access or breaches. Colleagues are required to report any concerns related to AI usage, including privacy breaches, bias, or ethical issues, to management or a designated authority. Caution: Never share personal, sensitive, or confidential data with generative AI technologies. Any use of AI that COULD impact on personal data MUST have a DPIA completed Requirements for AI Generated Content Proofing: All AI-generated content must be proofread and checked for accuracy by a human before being published or shared. This includes checking for spelling errors, grammar mistakes, and factual inaccuracies. Editing: AI-generated content must be edited to ensure that it is well-written, coherent, and engaging. This includes ensuring that the content is structured in a logical manner and that it is appropriate for the intended audience, aligned to MHA’s brand guidelines and tone of voice. Fact-checking: AI-generated content must be fact-checked via a reputable source to ensure that all information is accurate and up to date. This includes verifying sources, checking statistics, and ensuring that any claims made in the content are supported by evidence. AI is the starting point, not the finished product.",
    "responsibilities": "Continuous Improvement MHA will regularly review and assess the impact of AI on the people who MHA support and make necessary adjustments to improve outcomes and ensure ethical use. Residents, members, families, representatives, and colleagues will have avenues to report concerns, issues, or suggestions related to AI use. Training and Monitoring Actions which are found to contravene those outlined in this policy will be referred into the appropriate process for resolution, which may include the Disciplinary or Probation Policy. Compliance is assessed through direct observation, monitoring, and supervision of our colleagues and associated systems. Communication and Dissemination This policy is disseminated and implemented within all MHA services through MHA’s channels of communication. Each colleague’s line manager must ensure that all teams are aware of their roles, responsibilities. This policy will be available to the people we support and their representatives in alternate formats, as required. Any review of this policy will include consultation with our colleagues, review of support planning, incident reports, quality audits and feedback from other agencies. Queries and issues relating to this policy should be referred to the Standards and Policy Team policies@mha.org.uk Resources MHA policy documents, procedures, and guidance: Risk Management Policy Computer Use Policy Safeguarding Policy MHA’s Privacy Notices [Information Governance Policies] Consent Policy Disciplinary Policy Probation Policy Retention Schedule Guidance Appendices Appendix 1 – use of AI notetakers and secretarial tools Appendix 1 - Use of AI Notetakers and Secretarial Tools"
  }
}